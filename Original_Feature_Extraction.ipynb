{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22888704",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb8f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "ROOT_DIR = \"caverlee-2011/social_honeypot_icwsm_2011/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e598bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "\n",
    "ham_users = pd.read_csv(ROOT_DIR + \"legitimate_users.txt\",\n",
    "                         sep='\\t',\n",
    "                         names = ['UserID',\n",
    "                                  'CreatedAt',\n",
    "                                  'CollectedAt',\n",
    "                                  'NumberOfFollowings',\n",
    "                                  'NumberOfFollowers',\n",
    "                                  'NumberOfTweets',\n",
    "                                  'LengthOfScreenName',\n",
    "                                  'LengthOfDescriptionInUserProfile'])\n",
    "ham_tweets = pd.read_csv(ROOT_DIR + \"legitimate_users_tweets.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'TweetID',\n",
    "                                                'Tweet',\n",
    "                                                'CreatedAt'])\n",
    "\n",
    "ham_followings = pd.read_csv(ROOT_DIR + \"legitimate_users_followings.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'SeriesOfNumberOfFollowings'])\n",
    "\n",
    "\n",
    "spam_users = pd.read_csv(ROOT_DIR + \"content_polluters.txt\",\n",
    "                         sep='\\t',\n",
    "                         names = ['UserID',\n",
    "                                  'CreatedAt',\n",
    "                                  'CollectedAt',\n",
    "                                  'NumberOfFollowings',\n",
    "                                  'NumberOfFollowers',\n",
    "                                  'NumberOfTweets',\n",
    "                                  'LengthOfScreenName',\n",
    "                                  'LengthOfDescriptionInUserProfile'])\n",
    "spam_tweets = pd.read_csv(ROOT_DIR + \"content_polluters_tweets.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'TweetID',\n",
    "                                                'Tweet',\n",
    "                                                'CreatedAt'])\n",
    "spam_followings = pd.read_csv(ROOT_DIR + \"content_polluters_followings.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'SeriesOfNumberOfFollowings'])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57124967",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c4852",
   "metadata": {},
   "source": [
    "Features:\n",
    "\n",
    "(User demographics)\n",
    "1. Length of Screen Name (given)\n",
    "2. Length of Description (given)\n",
    "3. Longevity (calculate)\n",
    "\n",
    "(User Friendship Networks)\n",
    "1. Number of following (given)\n",
    "2. Number of followers (given)\n",
    "3. Ratio of Number of following and followers (calculate)\n",
    "4. Percentage of Bidirectional Friends (missing)\n",
    "5. Standard Deviation of Unique numerical IDs of following (missing)\n",
    "6. standard deviation of unique numerical IDs of followers (missing)\n",
    "\n",
    "(User Content)\n",
    "1. the number of posted tweets (given)\n",
    "2. number of posted tweets per day (calculate)\n",
    "3. |links|in tweets /|tweets| (calculate)\n",
    "4. |unique links|in tweets /|tweets| (calculate)\n",
    "5. |@username|in tweets /|tweets| (calculate)\n",
    "6. |unique@username|in tweets /|tweets| (calculate)\n",
    "7. Average Content Similarity over all pairs of tweets posted by a user (missing, don't know similarity metric)\n",
    "8. ZIP compression ratio of posted tweets (calculate)\n",
    "\n",
    "(User History)\n",
    "1. Change rate of number of following (calculate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38daa292",
   "metadata": {},
   "source": [
    "#### Given Features\n",
    "Length of Screen Name, Length of User Profile Description, Number of Following, Number of Followers, Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea331db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# given features\n",
    "ham_users_input = ham_users[['UserID',\n",
    "                             'LengthOfScreenName',\n",
    "                            'LengthOfDescriptionInUserProfile',\n",
    "                           'NumberOfFollowings',\n",
    "                           'NumberOfFollowers',\n",
    "                           'NumberOfTweets']]\n",
    "\n",
    "spam_users_input = spam_users[['UserID',\n",
    "                             'LengthOfScreenName',\n",
    "                            'LengthOfDescriptionInUserProfile',\n",
    "                           'NumberOfFollowings',\n",
    "                           'NumberOfFollowers',\n",
    "                           'NumberOfTweets']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c06fa0",
   "metadata": {},
   "source": [
    "Extract number of Tweets recorded in the database per user. This will come in handy for calculations later down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13cff945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet counts by user\n",
    "ham_tweet_counts = ham_tweets.groupby('UserID').size().reset_index()\n",
    "ham_tweet_counts.columns = ['UserID','RecordedTweetCount']\n",
    "ham_users = pd.merge(ham_users, ham_tweet_counts, on='UserID', how='left')\n",
    "\n",
    "spam_tweet_counts = spam_tweets.groupby('UserID').size().reset_index()\n",
    "spam_tweet_counts.columns = ['UserID','RecordedTweetCount']\n",
    "spam_users = pd.merge(spam_users, spam_tweet_counts, on='UserID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d02a3",
   "metadata": {},
   "source": [
    "#### Longevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98380fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add longevity\n",
    "from datetime import datetime\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "def get_longevity(start,end):\n",
    "    start_date = datetime.strptime(start, date_format)\n",
    "    end_date = datetime.strptime(end, date_format)\n",
    "    return (end_date - start_date).days\n",
    "\n",
    "ham_users_input['Longevity'] = ham_users.apply(lambda row: get_longevity(row['CreatedAt'],row['CollectedAt']), axis=1)\n",
    "\n",
    "spam_users_input['Longevity'] = spam_users.apply(lambda row: get_longevity(row['CreatedAt'],row['CollectedAt']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e1e3c",
   "metadata": {},
   "source": [
    "#### Ratio of Following to Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c0032a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add ratio of followings to followers\n",
    "ham_users_input['FollowRatio'] = ham_users_input['NumberOfFollowings'] / ham_users_input['NumberOfFollowers']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['FollowRatio'] = ham_users_input['FollowRatio'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add ratio of followings to followers\n",
    "spam_users_input['FollowRatio'] = spam_users_input['NumberOfFollowings'] / spam_users_input['NumberOfFollowers']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['FollowRatio'] = spam_users_input['FollowRatio'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d7590",
   "metadata": {},
   "source": [
    "#### Tweets Posted Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd90234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of posted tweets per day\n",
    "ham_users_input['TweetsPerDay'] = ham_users_input['NumberOfTweets'] / ham_users_input['Longevity']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['TweetsPerDay'] = ham_users_input['TweetsPerDay'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add number of posted tweets per day\n",
    "spam_users_input['TweetsPerDay'] = spam_users_input['NumberOfTweets'] / spam_users_input['Longevity']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['TweetsPerDay'] = spam_users_input['TweetsPerDay'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370d0d8",
   "metadata": {},
   "source": [
    "#### Average Links Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa2accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average links per tweet\n",
    "import re\n",
    "\n",
    "# function for counting number of links in tweet\n",
    "def count_links(text):\n",
    "    return len(re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', str(text)))\n",
    "\n",
    "# add link count to tweets dataframe\n",
    "ham_tweets['LinkCount'] = ham_tweets['Tweet'].apply(lambda x: count_links(x))\n",
    "# get sum of links by user\n",
    "ham_sum_links = ham_tweets.groupby('UserID')['LinkCount'].sum().reset_index()\n",
    "ham_sum_links.columns = ['UserID','LinkSum']\n",
    "# add link average to features\n",
    "ham_users_input['LinkAverage'] = ham_sum_links['LinkSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['LinkAverage'] = ham_users_input['LinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add link count to tweets dataframe\n",
    "spam_tweets['LinkCount'] = spam_tweets['Tweet'].apply(lambda x: count_links(x))\n",
    "# get sum of links by user\n",
    "spam_sum_links = spam_tweets.groupby('UserID')['LinkCount'].sum().reset_index()\n",
    "spam_sum_links.columns = ['UserID','LinkSum']\n",
    "# add link average to features\n",
    "spam_users_input['LinkAverage'] = spam_sum_links['LinkSum'] / spam_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['LinkAverage'] = spam_users_input['LinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faad5b",
   "metadata": {},
   "source": [
    "#### Average Unique Links Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402000ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average unique links per tweet\n",
    "\n",
    "# function for finding links in tweet\n",
    "def get_links(text):\n",
    "    return set(re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', str(text)))\n",
    "\n",
    "# function for merging sets\n",
    "def merge_sets(sets):\n",
    "    return set.union(*sets)\n",
    "\n",
    "# add unique links to tweets dataframe\n",
    "ham_tweets['UniqueLinks'] = ham_tweets['Tweet'].apply(lambda x: get_links(x))\n",
    "# get unique links by user\n",
    "ham_unique_links = ham_tweets.groupby('UserID')['UniqueLinks'].agg(merge_sets).reset_index()\n",
    "ham_unique_links.columns = ['UserID','UniqueLinks']\n",
    "ham_unique_links['UniqueLinkSum'] = ham_unique_links['UniqueLinks'].apply(lambda x: len(x))\n",
    "# add unique link average to features\n",
    "ham_users_input['UniqueLinkAverage'] = ham_unique_links['UniqueLinkSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['UniqueLinkAverage'] = ham_users_input['UniqueLinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add unique links to tweets dataframe\n",
    "spam_tweets['UniqueLinks'] = spam_tweets['Tweet'].apply(lambda x: get_links(x))\n",
    "# get unique links by user\n",
    "spam_unique_links = spam_tweets.groupby('UserID')['UniqueLinks'].agg(merge_sets).reset_index()\n",
    "spam_unique_links.columns = ['UserID','UniqueLinks']\n",
    "spam_unique_links['UniqueLinkSum'] = spam_unique_links['UniqueLinks'].apply(lambda x: len(x))\n",
    "# add unique link average to features\n",
    "spam_users_input['UniqueLinkAverage'] = spam_unique_links['UniqueLinkSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['UniqueLinkAverage'] = spam_users_input['UniqueLinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb5b00",
   "metadata": {},
   "source": [
    "#### Average Mentions Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d61e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add average mentions per tweet\n",
    "\n",
    "def count_mentions(text):\n",
    "    return len(re.findall(\"@([a-zA-Z0-9]{1,15})\", str(text)))\n",
    "\n",
    "# add username count to tweets dataframe\n",
    "ham_tweets['MentionCount'] = ham_tweets['Tweet'].apply(lambda x: count_mentions(x))\n",
    "# get sum of mentions by user\n",
    "ham_sum_mentions = ham_tweets.groupby('UserID')['MentionCount'].sum().reset_index()\n",
    "ham_sum_mentions.columns = ['UserID','MentionSum']\n",
    "# add mention average to features\n",
    "ham_users_input['MentionAverage'] = ham_sum_mentions['MentionSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['MentionAverage'] = ham_users_input['MentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add username count to tweets dataframe\n",
    "spam_tweets['MentionCount'] = spam_tweets['Tweet'].apply(lambda x: count_mentions(x))\n",
    "# get sum of mentions by user\n",
    "spam_sum_mentions = spam_tweets.groupby('UserID')['MentionCount'].sum().reset_index()\n",
    "spam_sum_mentions.columns = ['UserID','MentionSum']\n",
    "# add mention average to features\n",
    "spam_users_input['MentionAverage'] = spam_sum_mentions['MentionSum'] / spam_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['MentionAverage'] = spam_users_input['MentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc170c2",
   "metadata": {},
   "source": [
    "#### Average Unique Mentions Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176893d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average unique mentions per tweet\n",
    "\n",
    "# function for finding mentions in tweet\n",
    "def get_mentions(text):\n",
    "    return set(re.findall(\"@([a-zA-Z0-9]{1,15})\", str(text)))\n",
    "\n",
    "# add unique mentions to tweets dataframe\n",
    "ham_tweets['UniqueMentions'] = ham_tweets['Tweet'].apply(lambda x: get_mentions(x))\n",
    "# get unique mentions by user\n",
    "ham_unique_mentions = ham_tweets.groupby('UserID')['UniqueMentions'].agg(merge_sets).reset_index()\n",
    "ham_unique_mentions.columns = ['UserID','UniqueMentions']\n",
    "ham_unique_mentions['UniqueMentionSum'] = ham_unique_mentions['UniqueMentions'].apply(lambda x: len(x))\n",
    "# add unique mention average to features\n",
    "ham_users_input['UniqueMentionAverage'] = ham_unique_mentions['UniqueMentionSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['UniqueMentionAverage'] = ham_users_input['UniqueMentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add unique mentions to tweets dataframe\n",
    "spam_tweets['UniqueMentions'] = spam_tweets['Tweet'].apply(lambda x: get_mentions(x))\n",
    "# get unique mentions by user\n",
    "spam_unique_mentions = spam_tweets.groupby('UserID')['UniqueMentions'].agg(merge_sets).reset_index()\n",
    "spam_unique_mentions.columns = ['UserID','UniqueMentions']\n",
    "spam_unique_mentions['UniqueMentionSum'] = spam_unique_mentions['UniqueMentions'].apply(lambda x: len(x))\n",
    "# add unique mention average to features\n",
    "spam_users_input['UniqueMentionAverage'] = spam_unique_mentions['UniqueMentionSum'] / spam_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['UniqueMentionAverage'] = spam_users_input['UniqueMentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f51e1",
   "metadata": {},
   "source": [
    "#### Zip Compression Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02bdd91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add zip compression ratio\n",
    "\n",
    "import zlib\n",
    "\n",
    "# get compression ratio of tweets\n",
    "def get_compression_ratio(text):\n",
    "    data=text.encode('utf-8')\n",
    "    compressed_data = zlib.compress(data)\n",
    "    return len(data)/len(compressed_data)\n",
    "\n",
    "# concatenate tweets by user\n",
    "ham_concat_tweets = ham_tweets.groupby('UserID')['Tweet'].agg(lambda x: ' '.join(str(x))).reset_index()\n",
    "ham_concat_tweets.columns = ['UserID','Concat_Tweets']\n",
    "# calculate compression ratio from concatenated tweets\n",
    "ham_concat_tweets['Compression_Ratio'] = ham_concat_tweets['Concat_Tweets'].apply(lambda x: get_compression_ratio(x))\n",
    "# add to features\n",
    "ham_users_input = pd.merge(ham_users_input, ham_concat_tweets[['UserID','Compression_Ratio']], on='UserID', how='left')\n",
    "# give users with no tweets compression ratio of 0\n",
    "ham_users_input['Compression_Ratio'] = ham_users_input['Compression_Ratio'].fillna(0)\n",
    "\n",
    "# concatenate tweets by user\n",
    "spam_concat_tweets = spam_tweets.groupby('UserID')['Tweet'].agg(lambda x: ' '.join(str(x))).reset_index()\n",
    "spam_concat_tweets.columns = ['UserID','Concat_Tweets']\n",
    "# calculate compression ratio from concatenated tweets\n",
    "spam_concat_tweets['Compression_Ratio'] = spam_concat_tweets['Concat_Tweets'].apply(lambda x: get_compression_ratio(x))\n",
    "# add to features\n",
    "spam_users_input = pd.merge(spam_users_input, spam_concat_tweets[['UserID','Compression_Ratio']], on='UserID', how='left')\n",
    "# give users with no tweets compression ratio of 0\n",
    "spam_users_input['Compression_Ratio'] = spam_users_input['Compression_Ratio'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b60a36",
   "metadata": {},
   "source": [
    "#### Following Change Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b5de4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add following change rate\n",
    "import math\n",
    "\n",
    "# calculate change rate based on sequence string\n",
    "def calculate_change_rate(seq_str):\n",
    "    seq = [int(x) for x in seq_str.split(\",\")]\n",
    "    n = len(seq)\n",
    "    # perform summation\n",
    "    total =0\n",
    "    for i in range(0,n-1):\n",
    "        total += seq[i+1] - seq[i]\n",
    "    return total/(n-1)\n",
    "#     return math.sqrt(total/(n-1))\n",
    "\n",
    "# calculate following change rate by user\n",
    "ham_followings['FollowingChangeRate'] = ham_followings['SeriesOfNumberOfFollowings'].apply(lambda x: calculate_change_rate(x))\n",
    "# add to features\n",
    "ham_users_input = pd.merge(ham_users_input, ham_followings[['UserID','FollowingChangeRate']], on='UserID', how='left')\n",
    "# fill in zeroes\n",
    "ham_users_input['FollowingChangeRate'] = ham_users_input['FollowingChangeRate'].fillna(0)\n",
    "\n",
    "# calculate following change rate by user\n",
    "spam_followings['FollowingChangeRate'] = spam_followings['SeriesOfNumberOfFollowings'].apply(lambda x: calculate_change_rate(x))\n",
    "# add to features\n",
    "spam_users_input = pd.merge(spam_users_input, spam_followings[['UserID','FollowingChangeRate']], on='UserID', how='left')\n",
    "# fill in zeroes\n",
    "spam_users_input['FollowingChangeRate'] = spam_users_input['FollowingChangeRate'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b66ac",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7166716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels\n",
    "ham_users_input['label'] = 0 \n",
    "spam_users_input['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14decd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dataframes\n",
    "df = pd.concat([ham_users_input,spam_users_input])\n",
    "# drop userID\n",
    "df.drop(['UserID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebad6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "X = df.drop('label', axis = 1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b49a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8d5da",
   "metadata": {},
   "source": [
    "### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72093d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=23)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train RF\n",
    "clf = RandomForestClassifier(random_state=23)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157c281",
   "metadata": {},
   "source": [
    "### Testing Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e333cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

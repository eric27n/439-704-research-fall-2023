{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22888704",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb8f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "ROOT_DIR = \"caverlee-2011/social_honeypot_icwsm_2011/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e598bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "\n",
    "ham_users = pd.read_csv(ROOT_DIR + \"legitimate_users.txt\",\n",
    "                         sep='\\t',\n",
    "                         names = ['UserID',\n",
    "                                  'CreatedAt',\n",
    "                                  'CollectedAt',\n",
    "                                  'NumberOfFollowings',\n",
    "                                  'NumberOfFollowers',\n",
    "                                  'NumberOfTweets',\n",
    "                                  'LengthOfScreenName',\n",
    "                                  'LengthOfDescriptionInUserProfile'])\n",
    "ham_tweets = pd.read_csv(ROOT_DIR + \"legitimate_users_tweets.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'TweetID',\n",
    "                                                'Tweet',\n",
    "                                                'CreatedAt'])\n",
    "\n",
    "ham_followings = pd.read_csv(ROOT_DIR + \"legitimate_users_followings.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'SeriesOfNumberOfFollowings'])\n",
    "\n",
    "\n",
    "spam_users = pd.read_csv(ROOT_DIR + \"content_polluters.txt\",\n",
    "                         sep='\\t',\n",
    "                         names = ['UserID',\n",
    "                                  'CreatedAt',\n",
    "                                  'CollectedAt',\n",
    "                                  'NumberOfFollowings',\n",
    "                                  'NumberOfFollowers',\n",
    "                                  'NumberOfTweets',\n",
    "                                  'LengthOfScreenName',\n",
    "                                  'LengthOfDescriptionInUserProfile'])\n",
    "spam_tweets = pd.read_csv(ROOT_DIR + \"content_polluters_tweets.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'TweetID',\n",
    "                                                'Tweet',\n",
    "                                                'CreatedAt'])\n",
    "spam_followings = pd.read_csv(ROOT_DIR + \"content_polluters_followings.txt\",\n",
    "                                       sep='\\t',\n",
    "                                       names = ['UserID',\n",
    "                                                'SeriesOfNumberOfFollowings'])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bbaeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of content polluters: 22223\n",
      "Num of content polluter's tweets: 2333691\n",
      "Num of legitimate users: 19276\n",
      "Num of legitimate user's tweets: 3246377\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of content polluters: \" + str(len(spam_users)))\n",
    "print(\"Num of content polluter's tweets: \" + str(len(spam_tweets)))\n",
    "print(\"Num of legitimate users: \" + str(len(ham_users)))\n",
    "print(\"Num of legitimate user's tweets: \" + str(len(ham_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57124967",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c4852",
   "metadata": {},
   "source": [
    "Features:\n",
    "\n",
    "(User demographics)\n",
    "1. Length of Screen Name (given)\n",
    "2. Length of Description (given)\n",
    "3. Longevity (calculate)\n",
    "\n",
    "(User Friendship Networks)\n",
    "1. Number of following (given)\n",
    "2. Number of followers (given)\n",
    "3. Ratio of Number of following and followers (calculate)\n",
    "4. Percentage of Bidirectional Friends (missing)\n",
    "5. Standard Deviation of Unique numerical IDs of following (missing)\n",
    "6. standard deviation of unique numerical IDs of followers (missing)\n",
    "\n",
    "(User Content)\n",
    "1. the number of posted tweets (given)\n",
    "2. number of posted tweets per day (calculate)\n",
    "3. |links|in tweets /|tweets| (calculate)\n",
    "4. |unique links|in tweets /|tweets| (calculate)\n",
    "5. |@username|in tweets /|tweets| (calculate)\n",
    "6. |unique@username|in tweets /|tweets| (calculate)\n",
    "7. Average Content Similarity over all pairs of tweets posted by a user (calculate, don't know similarity metric- will use jaccard)\n",
    "8. ZIP compression ratio of posted tweets (calculate)\n",
    "\n",
    "(User History)\n",
    "1. Change rate of number of following (calculate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38daa292",
   "metadata": {},
   "source": [
    "#### Given Features\n",
    "Length of Screen Name, Length of User Profile Description, Number of Following, Number of Followers, Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea331db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# given features\n",
    "ham_users_input = ham_users[['UserID',\n",
    "                             'LengthOfScreenName',\n",
    "                            'LengthOfDescriptionInUserProfile',\n",
    "                           'NumberOfFollowings',\n",
    "                           'NumberOfFollowers',\n",
    "                           'NumberOfTweets']]\n",
    "\n",
    "spam_users_input = spam_users[['UserID',\n",
    "                             'LengthOfScreenName',\n",
    "                            'LengthOfDescriptionInUserProfile',\n",
    "                           'NumberOfFollowings',\n",
    "                           'NumberOfFollowers',\n",
    "                           'NumberOfTweets']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c06fa0",
   "metadata": {},
   "source": [
    "Extract number of Tweets recorded in the database per user. This will come in handy for calculations later down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13cff945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet counts by user\n",
    "ham_tweet_counts = ham_tweets.groupby('UserID').size().reset_index()\n",
    "ham_tweet_counts.columns = ['UserID','RecordedTweetCount']\n",
    "ham_users = pd.merge(ham_users, ham_tweet_counts, on='UserID', how='left')\n",
    "\n",
    "spam_tweet_counts = spam_tweets.groupby('UserID').size().reset_index()\n",
    "spam_tweet_counts.columns = ['UserID','RecordedTweetCount']\n",
    "spam_users = pd.merge(spam_users, spam_tweet_counts, on='UserID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d02a3",
   "metadata": {},
   "source": [
    "#### Longevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98380fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add longevity\n",
    "from datetime import datetime\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "def get_longevity(start,end):\n",
    "    start_date = datetime.strptime(start, date_format)\n",
    "    end_date = datetime.strptime(end, date_format)\n",
    "    return (end_date - start_date).days\n",
    "\n",
    "ham_users_input['Longevity'] = ham_users.apply(lambda row: get_longevity(row['CreatedAt'],row['CollectedAt']), axis=1)\n",
    "\n",
    "spam_users_input['Longevity'] = spam_users.apply(lambda row: get_longevity(row['CreatedAt'],row['CollectedAt']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e1e3c",
   "metadata": {},
   "source": [
    "#### Ratio of Following to Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c0032a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add ratio of followings to followers\n",
    "ham_users_input['FollowRatio'] = ham_users_input['NumberOfFollowings'] / ham_users_input['NumberOfFollowers']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['FollowRatio'] = ham_users_input['FollowRatio'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add ratio of followings to followers\n",
    "spam_users_input['FollowRatio'] = spam_users_input['NumberOfFollowings'] / spam_users_input['NumberOfFollowers']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['FollowRatio'] = spam_users_input['FollowRatio'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d7590",
   "metadata": {},
   "source": [
    "#### Tweets Posted Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd90234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of posted tweets per day\n",
    "ham_users_input['TweetsPerDay'] = ham_users_input['NumberOfTweets'] / ham_users_input['Longevity']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['TweetsPerDay'] = ham_users_input['TweetsPerDay'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add number of posted tweets per day\n",
    "spam_users_input['TweetsPerDay'] = spam_users_input['NumberOfTweets'] / spam_users_input['Longevity']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['TweetsPerDay'] = spam_users_input['TweetsPerDay'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370d0d8",
   "metadata": {},
   "source": [
    "#### Average Links Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa2accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average links per tweet\n",
    "import re\n",
    "\n",
    "# function for counting number of links in tweet\n",
    "def count_links(text):\n",
    "    return len(re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', str(text)))\n",
    "\n",
    "# add link count to tweets dataframe\n",
    "ham_tweets['LinkCount'] = ham_tweets['Tweet'].apply(lambda x: count_links(x))\n",
    "# get sum of links by user\n",
    "ham_sum_links = ham_tweets.groupby('UserID')['LinkCount'].sum().reset_index()\n",
    "ham_sum_links.columns = ['UserID','LinkSum']\n",
    "# add link average to features\n",
    "ham_users_input['LinkAverage'] = ham_sum_links['LinkSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['LinkAverage'] = ham_users_input['LinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add link count to tweets dataframe\n",
    "spam_tweets['LinkCount'] = spam_tweets['Tweet'].apply(lambda x: count_links(x))\n",
    "# get sum of links by user\n",
    "spam_sum_links = spam_tweets.groupby('UserID')['LinkCount'].sum().reset_index()\n",
    "spam_sum_links.columns = ['UserID','LinkSum']\n",
    "# add link average to features\n",
    "spam_users_input['LinkAverage'] = spam_sum_links['LinkSum'] / spam_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['LinkAverage'] = spam_users_input['LinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faad5b",
   "metadata": {},
   "source": [
    "#### Average Unique Links Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402000ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average unique links per tweet\n",
    "\n",
    "# function for finding links in tweet\n",
    "def get_links(text):\n",
    "    return set(re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', str(text)))\n",
    "\n",
    "# function for merging sets\n",
    "def merge_sets(sets):\n",
    "    return set.union(*sets)\n",
    "\n",
    "# add unique links to tweets dataframe\n",
    "ham_tweets['UniqueLinks'] = ham_tweets['Tweet'].apply(lambda x: get_links(x))\n",
    "# get unique links by user\n",
    "ham_unique_links = ham_tweets.groupby('UserID')['UniqueLinks'].agg(merge_sets).reset_index()\n",
    "ham_unique_links.columns = ['UserID','UniqueLinks']\n",
    "ham_unique_links['UniqueLinkSum'] = ham_unique_links['UniqueLinks'].apply(lambda x: len(x))\n",
    "# add unique link average to features\n",
    "ham_users_input['UniqueLinkAverage'] = ham_unique_links['UniqueLinkSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['UniqueLinkAverage'] = ham_users_input['UniqueLinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add unique links to tweets dataframe\n",
    "spam_tweets['UniqueLinks'] = spam_tweets['Tweet'].apply(lambda x: get_links(x))\n",
    "# get unique links by user\n",
    "spam_unique_links = spam_tweets.groupby('UserID')['UniqueLinks'].agg(merge_sets).reset_index()\n",
    "spam_unique_links.columns = ['UserID','UniqueLinks']\n",
    "spam_unique_links['UniqueLinkSum'] = spam_unique_links['UniqueLinks'].apply(lambda x: len(x))\n",
    "# add unique link average to features\n",
    "spam_users_input['UniqueLinkAverage'] = spam_unique_links['UniqueLinkSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['UniqueLinkAverage'] = spam_users_input['UniqueLinkAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb5b00",
   "metadata": {},
   "source": [
    "#### Average Mentions Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d61e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add average mentions per tweet\n",
    "\n",
    "def count_mentions(text):\n",
    "    return len(re.findall(\"@([a-zA-Z0-9]{1,15})\", str(text)))\n",
    "\n",
    "# add username count to tweets dataframe\n",
    "ham_tweets['MentionCount'] = ham_tweets['Tweet'].apply(lambda x: count_mentions(x))\n",
    "# get sum of mentions by user\n",
    "ham_sum_mentions = ham_tweets.groupby('UserID')['MentionCount'].sum().reset_index()\n",
    "ham_sum_mentions.columns = ['UserID','MentionSum']\n",
    "# add mention average to features\n",
    "ham_users_input['MentionAverage'] = ham_sum_mentions['MentionSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['MentionAverage'] = ham_users_input['MentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add username count to tweets dataframe\n",
    "spam_tweets['MentionCount'] = spam_tweets['Tweet'].apply(lambda x: count_mentions(x))\n",
    "# get sum of mentions by user\n",
    "spam_sum_mentions = spam_tweets.groupby('UserID')['MentionCount'].sum().reset_index()\n",
    "spam_sum_mentions.columns = ['UserID','MentionSum']\n",
    "# add mention average to features\n",
    "spam_users_input['MentionAverage'] = spam_sum_mentions['MentionSum'] / spam_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['MentionAverage'] = spam_users_input['MentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc170c2",
   "metadata": {},
   "source": [
    "#### Average Unique Mentions Per Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176893d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average unique mentions per tweet\n",
    "\n",
    "# function for finding mentions in tweet\n",
    "def get_mentions(text):\n",
    "    return set(re.findall(\"@([a-zA-Z0-9]{1,15})\", str(text)))\n",
    "\n",
    "# add unique mentions to tweets dataframe\n",
    "ham_tweets['UniqueMentions'] = ham_tweets['Tweet'].apply(lambda x: get_mentions(x))\n",
    "# get unique mentions by user\n",
    "ham_unique_mentions = ham_tweets.groupby('UserID')['UniqueMentions'].agg(merge_sets).reset_index()\n",
    "ham_unique_mentions.columns = ['UserID','UniqueMentions']\n",
    "ham_unique_mentions['UniqueMentionSum'] = ham_unique_mentions['UniqueMentions'].apply(lambda x: len(x))\n",
    "# add unique mention average to features\n",
    "ham_users_input['UniqueMentionAverage'] = ham_unique_mentions['UniqueMentionSum'] / ham_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "ham_users_input['UniqueMentionAverage'] = ham_users_input['UniqueMentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# add unique mentions to tweets dataframe\n",
    "spam_tweets['UniqueMentions'] = spam_tweets['Tweet'].apply(lambda x: get_mentions(x))\n",
    "# get unique mentions by user\n",
    "spam_unique_mentions = spam_tweets.groupby('UserID')['UniqueMentions'].agg(merge_sets).reset_index()\n",
    "spam_unique_mentions.columns = ['UserID','UniqueMentions']\n",
    "spam_unique_mentions['UniqueMentionSum'] = spam_unique_mentions['UniqueMentions'].apply(lambda x: len(x))\n",
    "# add unique mention average to features\n",
    "spam_users_input['UniqueMentionAverage'] = spam_unique_mentions['UniqueMentionSum'] / spam_users['RecordedTweetCount']\n",
    "# replace divide by zero errors with 0\n",
    "spam_users_input['UniqueMentionAverage'] = spam_users_input['UniqueMentionAverage'].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a80f9",
   "metadata": {},
   "source": [
    "#### Average Similarity over all Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5a7a845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 19251\n",
      "2000 / 19251\n",
      "3000 / 19251\n",
      "4000 / 19251\n",
      "5000 / 19251\n",
      "6000 / 19251\n",
      "7000 / 19251\n",
      "8000 / 19251\n",
      "9000 / 19251\n",
      "10000 / 19251\n",
      "11000 / 19251\n",
      "12000 / 19251\n",
      "13000 / 19251\n",
      "14000 / 19251\n",
      "15000 / 19251\n",
      "16000 / 19251\n",
      "17000 / 19251\n",
      "18000 / 19251\n",
      "19000 / 19251\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25848/2215333186.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# add to features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mham_users_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mham_users_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mham_jaccard_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UserID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Similarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UserID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;31m# give users with no tweets similarity of 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mham_users_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Similarity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mham_users_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Similarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# add average similarity score for every pairwise comparisons\n",
    "import itertools\n",
    "\n",
    "# similarity calculation\n",
    "def jaccard(str1,str2): \n",
    "    # split words to sets\n",
    "    set1 = set(str1.lower().split()) \n",
    "    set2 = set(str2.lower().split())\n",
    "    # catch divide by 0\n",
    "    if len(set1.union(set2)) == 0:\n",
    "        return 1\n",
    "    # calculate jaccard (intersection / union)\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "# store ham jaccard scores\n",
    "ham_jaccard_scores = []\n",
    "\n",
    "ham_unique_ids = ham_tweets['UserID'].unique()\n",
    "count = 0\n",
    "for user in ham_unique_ids:\n",
    "    # get all tweets for user\n",
    "    x = list(ham_tweets.loc[ham_tweets['UserID'] == user]['Tweet'])\n",
    "    \n",
    "    if (len(x) > 1):\n",
    "        combos = list(itertools.combinations(x, 2))\n",
    "        # get average jaccard score over all pairs\n",
    "        sim_scores = [jaccard(str(pair[0]),str(pair[1])) for pair in combos]\n",
    "        avg_score = sum(sim_scores) / len(combos)\n",
    "        # record similarity score\n",
    "        ham_jaccard_scores.append(avg_score)\n",
    "    else:\n",
    "        ham_jaccard_scores.append(1)\n",
    "    # progress\n",
    "    count +=1\n",
    "    if count % 1000 == 0:\n",
    "        print(str(count) + \" / \" + str(len(ham_unique_ids)))\n",
    "\n",
    "# create dataframe of ids and scores\n",
    "ham_jaccard_dict = {'UserID' : ham_unique_ids, 'Similarity' : ham_jaccard_scores}\n",
    "ham_jaccard_df = pd.DataFrame.from_dict(ham_jaccard_dict)\n",
    "\n",
    "# add to features\n",
    "ham_users_input = pd.merge(ham_users_input, ham_jaccard_df[['UserID','Similarity']], on='UserID', how='left')\n",
    "# give users with no tweets similarity of 1\n",
    "ham_users_input['Similarity'] = ham_users_input['Similarity'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 20645\n",
      "2000 / 20645\n",
      "3000 / 20645\n"
     ]
    }
   ],
   "source": [
    "# store spam jaccard scores\n",
    "spam_jaccard_scores = []\n",
    "\n",
    "spam_unique_ids = spam_tweets['UserID'].unique()\n",
    "count = 0\n",
    "for user in spam_unique_ids:\n",
    "    # get all tweets for user\n",
    "    x = list(spam_tweets.loc[spam_tweets['UserID'] == user]['Tweet'])\n",
    "    \n",
    "    if (len(x) > 1):\n",
    "        combos = list(itertools.combinations(x, 2))\n",
    "        # get average jaccard score over all pairs\n",
    "        sim_scores = [jaccard(str(pair[0]),str(pair[1])) for pair in combos]\n",
    "        avg_score = sum(sim_scores) / len(combos)\n",
    "        # record similarity score\n",
    "        spam_jaccard_scores.append(avg_score)\n",
    "    else:\n",
    "        spam_jaccard_scores.append(1)\n",
    "    # progress\n",
    "    count +=1\n",
    "    if count % 1000 == 0:\n",
    "        print(str(count) + \" / \" + str(len(spam_unique_ids)))\n",
    "\n",
    "# create dataframe of ids and scores\n",
    "spam_jaccard_dict = {'UserID' : spam_unique_ids, 'Similarity' : spam_jaccard_scores}\n",
    "spam_jaccard_df = pd.DataFrame.from_dict(spam_jaccard_dict)\n",
    "\n",
    "# add to features\n",
    "spam_users_input = pd.merge(spam_users_input, spam_jaccard_df[['UserID','Similarity']], on='UserID', how='left')\n",
    "# give users with no tweets similarity of 1\n",
    "spam_users_input['Similarity'] = spam_users_input['Similarity'].fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f51e1",
   "metadata": {},
   "source": [
    "#### Zip Compression Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02bdd91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add zip compression ratio\n",
    "\n",
    "import zlib\n",
    "\n",
    "# get compression ratio of tweets\n",
    "def get_compression_ratio(text):\n",
    "    data=text.encode('utf-8')\n",
    "    compressed_data = zlib.compress(data)\n",
    "    return len(data)/len(compressed_data)\n",
    "\n",
    "# concatenate tweets by user\n",
    "ham_concat_tweets = ham_tweets.groupby('UserID')['Tweet'].agg(lambda x: ' '.join(str(x))).reset_index()\n",
    "ham_concat_tweets.columns = ['UserID','Concat_Tweets']\n",
    "# calculate compression ratio from concatenated tweets\n",
    "ham_concat_tweets['Compression_Ratio'] = ham_concat_tweets['Concat_Tweets'].apply(lambda x: get_compression_ratio(x))\n",
    "# add to features\n",
    "ham_users_input = pd.merge(ham_users_input, ham_concat_tweets[['UserID','Compression_Ratio']], on='UserID', how='left')\n",
    "# give users with no tweets compression ratio of 0\n",
    "ham_users_input['Compression_Ratio'] = ham_users_input['Compression_Ratio'].fillna(0)\n",
    "\n",
    "# concatenate tweets by user\n",
    "spam_concat_tweets = spam_tweets.groupby('UserID')['Tweet'].agg(lambda x: ' '.join(str(x))).reset_index()\n",
    "spam_concat_tweets.columns = ['UserID','Concat_Tweets']\n",
    "# calculate compression ratio from concatenated tweets\n",
    "spam_concat_tweets['Compression_Ratio'] = spam_concat_tweets['Concat_Tweets'].apply(lambda x: get_compression_ratio(x))\n",
    "# add to features\n",
    "spam_users_input = pd.merge(spam_users_input, spam_concat_tweets[['UserID','Compression_Ratio']], on='UserID', how='left')\n",
    "# give users with no tweets compression ratio of 0\n",
    "spam_users_input['Compression_Ratio'] = spam_users_input['Compression_Ratio'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b60a36",
   "metadata": {},
   "source": [
    "#### Following Change Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5de4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add following change rate\n",
    "import math\n",
    "\n",
    "# calculate change rate based on sequence string\n",
    "def calculate_change_rate(seq_str):\n",
    "    seq = [int(x) for x in seq_str.split(\",\")]\n",
    "    n = len(seq)\n",
    "    # perform summation\n",
    "    total =0\n",
    "    for i in range(0,n-1):\n",
    "        total += abs(seq[i+1] - seq[i])\n",
    "#     return total/(n-1)\n",
    "    return math.sqrt(total/(n-1))\n",
    "\n",
    "# calculate following change rate by user\n",
    "ham_followings['FollowingChangeRate'] = ham_followings['SeriesOfNumberOfFollowings'].apply(lambda x: calculate_change_rate(x))\n",
    "# add to features\n",
    "ham_users_input = pd.merge(ham_users_input, ham_followings[['UserID','FollowingChangeRate']], on='UserID', how='left')\n",
    "# fill in zeroes\n",
    "ham_users_input['FollowingChangeRate'] = ham_users_input['FollowingChangeRate'].fillna(0)\n",
    "\n",
    "# calculate following change rate by user\n",
    "spam_followings['FollowingChangeRate'] = spam_followings['SeriesOfNumberOfFollowings'].apply(lambda x: calculate_change_rate(x))\n",
    "# add to features\n",
    "spam_users_input = pd.merge(spam_users_input, spam_followings[['UserID','FollowingChangeRate']], on='UserID', how='left')\n",
    "# fill in zeroes\n",
    "spam_users_input['FollowingChangeRate'] = spam_users_input['FollowingChangeRate'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12754b",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "ham_users_input.to_csv('data/ham_features.csv', index = False)\n",
    "spam_users_input.to_csv('data/spam_features.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
